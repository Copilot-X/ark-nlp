{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/shencj/workspace/code/nlp/Frame/ark-nlp-0.0.5/')\n",
    "\n",
    "from ark_nlp.model.ner.w2ner_bert import W2NERBert\n",
    "from ark_nlp.model.ner.w2ner_bert import CrfBertConfig\n",
    "from ark_nlp.model.ner.w2ner_bert import Dataset\n",
    "from ark_nlp.model.ner.w2ner_bert import Task\n",
    "from ark_nlp.model.ner.w2ner_bert import get_default_model_optimizer\n",
    "from ark_nlp.factory.lr_scheduler import get_default_linear_schedule_with_warmup\n",
    "from ark_nlp.model.ner.w2ner_bert import Tokenizer\n",
    "from ark_nlp.factory.utils.seed import set_seed\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目录地址\n",
    "\n",
    "train_data_path = '/home/shencj/workspace/data/corpus/ner/cn/resume-zh/train.json'\n",
    "dev_data_path = '/home/shencj/workspace/data/corpus/ner/cn/resume-zh/dev.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、数据读入与处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.read_json(train_data_path)\n",
    "dev_data_df = pd.read_json(dev_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(train_data_df):\n",
    "    entities = []\n",
    "    for sentence, ners in zip(train_data_df['sentence'], train_data_df['ner']):\n",
    "        entity = []\n",
    "        for ner in ners:\n",
    "            entity_ = {}\n",
    "            entity_['start_idx'] = ner['index'][0]\n",
    "            entity_['end_idx'] = ner['index'][-1]\n",
    "            entity_['type'] = ner['type']\n",
    "            entity_['entity'] = ''.join(sentence[ner['index'][0]:ner['index'][-1] + 1])\n",
    "            entity.append(entity_)\n",
    "        entities.append(entity)\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['label'] = get_label(train_data_df)\n",
    "dev_data_df['label'] = get_label(dev_data_df)\n",
    "train_data_df['text'] = train_data_df['sentence'].apply(lambda x: ' '.join(x))\n",
    "dev_data_df['text'] = dev_data_df['sentence'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = train_data_df.loc[:,['text', 'label']]\n",
    "train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "dev_data_df = dev_data_df.loc[:,['text', 'label']]\n",
    "dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset = Dataset(train_data_df)\n",
    "ner_dev_dataset = Dataset(dev_data_df, categories=ner_train_dataset.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 词典创建和生成分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_len 不要小于180， 会报index错误\n",
    "tokenizer = Tokenizer(vocab='bert-base-chinese', max_seq_len=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset.convert_to_ids(tokenizer)\n",
    "ner_dev_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 二、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CrfBertConfig.from_pretrained('bert-base-chinese',\n",
    "                                         num_labels=len(ner_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 模型创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing W2NERBert: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing W2NERBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing W2NERBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of W2NERBert were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['dis_embs.weight', 'reg_embs.weight', 'encoder.weight_ih_l0', 'encoder.weight_hh_l0', 'encoder.bias_ih_l0', 'encoder.bias_hh_l0', 'encoder.weight_ih_l0_reverse', 'encoder.weight_hh_l0_reverse', 'encoder.bias_ih_l0_reverse', 'encoder.bias_hh_l0_reverse', 'convLayer.base.1.weight', 'convLayer.base.1.bias', 'convLayer.convs.0.weight', 'convLayer.convs.0.bias', 'convLayer.convs.1.weight', 'convLayer.convs.1.bias', 'convLayer.convs.2.weight', 'convLayer.convs.2.bias', 'predictor.mlp1.linear.weight', 'predictor.mlp1.linear.bias', 'predictor.mlp2.linear.weight', 'predictor.mlp2.linear.bias', 'predictor.biaffine.weight', 'predictor.mlp_rel.linear.weight', 'predictor.mlp_rel.linear.bias', 'predictor.linear.weight', 'predictor.linear.bias', 'cln.beta', 'cln.gamma', 'cln.beta_dense.weight', 'cln.gamma_dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dl_module = W2NERBert.from_pretrained('bert-base-chinese',\n",
    "                                    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 三、任务构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 任务参数和必要部件设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行次数\n",
    "num_epoches = 20\n",
    "# 源码 batch_size = 12\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "bert_params = set(dl_module.bert.parameters())\n",
    "other_params = list(set(dl_module.parameters()) - bert_params)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "params = [\n",
    "    {'params': [p for n, p in dl_module.bert.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'lr': 5e-6,\n",
    "     'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in dl_module.bert.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     'lr': 5e-6,\n",
    "     'weight_decay': 0.0},\n",
    "    {'params': other_params,\n",
    "     'lr': 1e-3,\n",
    "     'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "optimizer = AdamW(params, lr=1e-3, weight_decay=0.0)\n",
    "\n",
    "t_total = len(ner_train_dataset) // batch_size * num_epoches\n",
    "scheduler = get_default_linear_schedule_with_warmup(optimizer, t_total, warmup_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 任务创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task(dl_module, optimizer, 'ce', cude_device=0, scheduler=scheduler, grad_clip=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 100/637 [00:37<03:21,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/637],train loss is:1.041140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 200/637 [01:15<02:45,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/637],train loss is:0.565830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 300/637 [01:53<02:08,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/637],train loss is:0.402918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 400/637 [02:31<01:30,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/637],train loss is:0.310090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 500/637 [03:09<00:52,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/637],train loss is:0.251957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 600/637 [03:47<00:14,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[599/637],train loss is:0.212280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [04:01<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[0],train loss is:0.200581 \n",
      "\n",
      "928 1003 1497\n",
      "eval loss is 0.007458, precision is:0.9252243270189432, recall is:0.6199064796259185, f1_score is:0.7424000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 100/637 [00:38<03:25,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/637],train loss is:0.009265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 200/637 [01:16<02:56,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/637],train loss is:0.008254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 300/637 [01:55<02:08,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/637],train loss is:0.007770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 400/637 [02:34<01:33,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/637],train loss is:0.007112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 500/637 [03:12<00:52,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/637],train loss is:0.006578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 600/637 [03:51<00:14,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[599/637],train loss is:0.006182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [04:05<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[1],train loss is:0.006091 \n",
      "\n",
      "1401 1485 1497\n",
      "eval loss is 0.002324, precision is:0.9434343434343434, recall is:0.935871743486974, f1_score is:0.9396378269617707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 100/637 [00:38<03:27,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/637],train loss is:0.004318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 200/637 [01:17<02:47,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/637],train loss is:0.003763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 300/637 [01:55<02:08,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/637],train loss is:0.003324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 400/637 [02:34<01:32,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/637],train loss is:0.003201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 500/637 [03:12<00:52,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/637],train loss is:0.003046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 600/637 [03:51<00:14,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[599/637],train loss is:0.003006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [04:05<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[2],train loss is:0.002907 \n",
      "\n",
      "1433 1521 1497\n",
      "eval loss is 0.002097, precision is:0.9421433267587114, recall is:0.957247828991316, f1_score is:0.949635520212061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 100/637 [00:38<03:26,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/637],train loss is:0.001848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 200/637 [01:17<02:46,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/637],train loss is:0.001943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 300/637 [01:56<02:07,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/637],train loss is:0.002025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 400/637 [02:34<01:31,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/637],train loss is:0.002113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 500/637 [03:13<00:51,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/637],train loss is:0.002148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 600/637 [03:51<00:14,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[599/637],train loss is:0.002108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [04:06<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[3],train loss is:0.002083 \n",
      "\n",
      "1448 1529 1497\n",
      "eval loss is 0.001867, precision is:0.94702419882276, recall is:0.9672678690714763, f1_score is:0.9570389953734303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 100/637 [00:38<03:32,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/637],train loss is:0.001360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 200/637 [01:17<02:50,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/637],train loss is:0.001409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 300/637 [01:56<02:09,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/637],train loss is:0.001655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 400/637 [02:34<01:29,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/637],train loss is:0.001644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 500/637 [03:13<00:52,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/637],train loss is:0.001593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 600/637 [03:52<00:14,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[599/637],train loss is:0.001594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 628/637 [04:03<00:03,  2.57it/s]"
     ]
    }
   ],
   "source": [
    "model.fit(ner_train_dataset,\n",
    "          ner_dev_dataset,\n",
    "          epochs=num_epoches,\n",
    "          batch_size=batch_size\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [transformers]",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
