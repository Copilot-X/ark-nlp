{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import codecs\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from ark_nlp.model.rc.pure_bert import PUREBert\n",
    "from ark_nlp.model.rc.pure_bert import PUREBertConfig\n",
    "from ark_nlp.model.rc.pure_bert import Dataset\n",
    "from ark_nlp.model.rc.pure_bert import Task\n",
    "from ark_nlp.model.rc.pure_bert import get_default_model_optimizer\n",
    "from ark_nlp.model.rc.pure_bert import Tokenizer\n",
    "from ark_nlp.factory.utils.seed import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、数据读入与处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目录地址\n",
    "\n",
    "train_data_path = '../data/source_datasets/CMeIE/CMeIE_train.json'\n",
    "dev_data_path = '../data/source_datasets/CMeIE/CMeIE_dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data_path):\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    with codecs.open(data_path, mode='r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        for index_, line_ in enumerate(lines):\n",
    "            record_ = {}\n",
    "            line_ = json.loads(line_.strip())\n",
    "            record_['text'] = line_['text']\n",
    "            record_['entities'] = []\n",
    "            record_['triples'] = []\n",
    "            for triple_ in line_['spo_list']:\n",
    "                record_['entities'].append([\n",
    "                    triple_['subject'],\n",
    "                    '疾病',\n",
    "                    record_['text'].index(triple_['subject']),\n",
    "                    record_['text'].index(triple_['subject'])+ len(triple_['subject']) - 1,\n",
    "                ])\n",
    "                record_['entities'].append([\n",
    "                    triple_['object']['@value'],\n",
    "                    triple_['object_type']['@value'],\n",
    "                    record_['text'].index(triple_['object']['@value']),\n",
    "                    record_['text'].index(triple_['object']['@value']) + len(triple_['object']['@value']) - 1,\n",
    "                ])\n",
    "                record_['triples'].append([\n",
    "                    triple_['subject'],\n",
    "                    '疾病',\n",
    "                    record_['text'].index(triple_['subject']),\n",
    "                    record_['text'].index(triple_['subject'])+ len(triple_['subject']) - 1,\n",
    "                    triple_['predicate'],\n",
    "                    triple_['object']['@value'],\n",
    "                    triple_['object_type']['@value'],\n",
    "                    record_['text'].index(triple_['object']['@value']),\n",
    "                    record_['text'].index(triple_['object']['@value']) + len(triple_['object']['@value']) - 1,\n",
    "                ])\n",
    "            record_['entities'] = list(set([tuple(entity) for entity in record_['entities']]))\n",
    "            record_['entities'] = sorted(record_['entities'], key = lambda x: x[2])\n",
    "            data_list.append(record_)\n",
    "    return data_list\n",
    "\n",
    "train_data_list = data_preprocess(train_data_path)\n",
    "train_df = pd.DataFrame(train_data_list)\n",
    "\n",
    "dev_data_list = data_preprocess(dev_data_path)\n",
    "dev_df = pd.DataFrame(dev_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 没有分类标为\"None\"\n",
    "categories = list(set([triple[4] for triples in train_df['triples'] for triple in triples])) + ['None']\n",
    "\n",
    "rc_train_dataset = Dataset(train_df, categories=categories)\n",
    "rc_dev_dataset = Dataset(dev_df, categories=categories,\n",
    "                         is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 词典创建和生成分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "bert_vocab = AutoTokenizer.from_pretrained('nghuyong/ernie-1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_categories = list(set([entity[1] for entities in train_df['entities'] for entity in entities]))\n",
    "special_tokens = []\n",
    "for category in entity_categories:\n",
    "    special_tokens.append(f'[{category}]')\n",
    "    special_tokens.append(f'[/{category}]')\n",
    "bert_vocab.add_special_tokens({'additional_special_tokens': special_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(bert_vocab, max_seq_len=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_train_dataset.convert_to_ids(tokenizer)\n",
    "rc_dev_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 二、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = PUREBertConfig.from_pretrained('nghuyong/ernie-1.0',\n",
    "                                               num_labels=len(rc_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 模型创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_module = PUREBert.from_pretrained('nghuyong/ernie-1.0',\n",
    "                                       config=bert_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 三、任务构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 任务参数和必要部件设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_default_model_optimizer(dl_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 任务创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task(dl_module, optimizer, 'ce', cuda_device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    rc_train_dataset,\n",
    "    rc_dev_dataset,\n",
    "    lr=2e-5,\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 四、模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from ark_nlp.model.rc.pure_bert import Predictor\n",
    "\n",
    "pure_rc_predictor_instance = Predictor(model.module, tokenizer, rc_train_dataset.cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '急性胰腺炎@有研究显示，进行早期 ERCP （24 小时内）可以降低梗阻性胆总管结石患者的并发症发生率和死亡率； 但是，对于无胆总管梗阻的胆汁性急性胰腺炎患者，不需要进行早期 ERCP。'\n",
    "entities = [('急性胰腺炎', '疾病', 0, 4), ('ERCP', '检查', 17, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_rc_predictor_instance.predict_one_sample(text, entities, topk=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 五、CMeIE结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_json('../data/output_datasets/CMeIE_test_entities.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['entities'] = test_df['entities'].apply(lambda x: [(entity['entity'], entity['type'], int(entity['start_idx']), int(entity['end_idx'])) for entity in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for text, entities in tqdm(zip(test_df['text'], test_df['entities'])):\n",
    "    result.append(pure_rc_predictor_instance.predict_one_sample(text, entities, topk=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['triples'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ = []\n",
    "for triples in result:\n",
    "    \n",
    "    triples_ = []\n",
    "    for triple in triples:\n",
    "        \n",
    "        if triple[1] == 'None' or triple[0][1] != '疾病':\n",
    "            continue\n",
    "        predicate = triple[1] + '@' + triple[2][1]\n",
    "        \n",
    "        triples_.append((triple[0][0], predicate, triple[2][0]))\n",
    "    result_.append(triples_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = '../data/source_datasets/CMeIE/CMeIE_test.json'\n",
    "schemas_data_path = '../data/source_datasets/CMeIE/53_schemas.json'\n",
    "output_data_path = '../data/output_datasets/CMeIE_test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subject_type = []\n",
    "all_predicate = []\n",
    "all_shcemas = []\n",
    "predicate2subject = {}\n",
    "with open(schemas_data_path, 'r', encoding='utf-8') as fs:\n",
    "    for jsonstr in fs.readlines():\n",
    "        jsonstr = json.loads(jsonstr)\n",
    "        # all_shcemas.append(jsonstr)\n",
    "        \n",
    "        predicate2subject[jsonstr['predicate']+'@'+jsonstr['object_type']] = jsonstr['subject_type']\n",
    "        \n",
    "    fs.close()\n",
    "    \n",
    "all_predicate = set(all_predicate)\n",
    "with open(output_data_path, 'w', encoding='utf-8') as fw:\n",
    "    with open(test_data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for index_, jsonstr in tqdm(enumerate(lines)):\n",
    "            line = json.loads(jsonstr)\n",
    "            results_len = []\n",
    "            sentence = line['text']\n",
    "            dict_list = result_[index_]\n",
    "            new = []\n",
    "            for list_ in dict_list:\n",
    "                for predicate_ in predicate2subject:\n",
    "                    if list_[1] == predicate_:\n",
    "                        if list_[-1] != '' and list_[-1] != '[UNK]':\n",
    "                            result_dict = {\n",
    "                                'predicate': predicate_.split('@')[0],\n",
    "                                \"subject\": list_[0],\n",
    "                                'subject_type': predicate2subject[predicate_],\n",
    "                                \"object\": {\"@value\": list_[-1]},\n",
    "                                'object_type': {\"@value\":predicate_.split('@')[-1]}\n",
    "                                }\n",
    "                        else:\n",
    "                            continue\n",
    "                        if result_dict not in new:\n",
    "                            new.append(result_dict)\n",
    "            if sum([item.count('。') for item in sentence]) >= 2:\n",
    "                for item in new:\n",
    "                    item['Combined'] = True\n",
    "            else:\n",
    "                for item in new:\n",
    "                    item['Combined'] = False\n",
    "\n",
    "            if len(new) == 0:\n",
    "                new = [{\n",
    "                    \"Combined\": '',\n",
    "                    \"predicate\": '',\n",
    "                    \"subject\": '',\n",
    "                    \"subject_type\": '',\n",
    "                    \"object\": {\"@value\": \"\"},\n",
    "                    \"object_type\": {\"@value\": \"\"},\n",
    "                }]\n",
    "                pred_dict = {\n",
    "                    \"text\": ''.join(sentence),\n",
    "                    \"spo_list\": new,\n",
    "                }\n",
    "            else:\n",
    "\n",
    "                pred_dict = {\n",
    "                    \"text\": ''.join(sentence),\n",
    "                    \"spo_list\": new,\n",
    "                }\n",
    "            fw.write(json.dumps(pred_dict, ensure_ascii=False) + '\\n')\n",
    "f.close()\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [transformers]",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}