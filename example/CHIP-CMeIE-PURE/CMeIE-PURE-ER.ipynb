{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import codecs\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from ark_nlp.model.ner.global_pointer_bert import GlobalPointerBert\n",
    "from ark_nlp.model.ner.global_pointer_bert import GlobalPointerBertConfig\n",
    "from ark_nlp.model.ner.global_pointer_bert import Dataset\n",
    "from ark_nlp.model.ner.global_pointer_bert import Task\n",
    "from ark_nlp.model.ner.global_pointer_bert import get_default_model_optimizer\n",
    "from ark_nlp.model.ner.global_pointer_bert import Tokenizer\n",
    "from ark_nlp.factory.utils.seed import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目录地址\n",
    "\n",
    "train_data_path = '../../data/source_datasets/CMeIE/CMeIE_train.json'\n",
    "dev_data_path = '../../data/source_datasets/CMeIE/CMeIE_dev.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、数据读入与处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data_path):\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    with codecs.open(data_path, mode='r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        for index_, line_ in enumerate(lines):\n",
    "            record_ = {}\n",
    "            line_ = json.loads(line_.strip())\n",
    "            record_['text'] = line_['text']\n",
    "            record_['entities'] = []\n",
    "            for triple_ in line_['spo_list']:\n",
    "                record_['entities'].append([\n",
    "                    triple_['subject'],\n",
    "                    '疾病',\n",
    "                    record_['text'].index(triple_['subject']),\n",
    "                    record_['text'].index(triple_['subject'])+ len(triple_['subject']) - 1,\n",
    "                ])\n",
    "                record_['entities'].append([\n",
    "                    triple_['object']['@value'],\n",
    "                    triple_['object_type']['@value'],\n",
    "                    record_['text'].index(triple_['object']['@value']),\n",
    "                    record_['text'].index(triple_['object']['@value']) + len(triple_['object']['@value']) - 1,\n",
    "                ])\n",
    "            record_['entities'] = list(set([tuple(entity) for entity in record_['entities']]))\n",
    "            record_['entities'] = sorted(record_['entities'], key = lambda x: x[2])\n",
    "            record_['label'] = [{'entity': entity_[0], 'type': entity_[1], 'start_idx': entity_[2], 'end_idx': entity_[3]} for entity_ in record_['entities']]\n",
    "            data_list.append(record_)\n",
    "    return data_list\n",
    "\n",
    "train_data_list = data_preprocess(train_data_path)\n",
    "train_data_df = pd.DataFrame(train_data_list)\n",
    "\n",
    "dev_data_list = data_preprocess(dev_data_path)\n",
    "dev_data_df = pd.DataFrame(dev_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset = Dataset(train_data_df)\n",
    "ner_dev_dataset = Dataset(dev_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 词典创建和生成分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以先创建词典，再加载入分词器\n",
    "# 也可以使用分词器自动加载\n",
    "# bert_vocab = transformers.AutoTokenizer.from_pretrained('nghuyong/ernie-1.0')\n",
    "# tokenizer = TransfomerTokenizer(bert_vocab, max_seq_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab='nghuyong/ernie-1.0', max_seq_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset.convert_to_ids(tokenizer)\n",
    "ner_dev_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 二、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GlobalPointerBertConfig.from_pretrained('nghuyong/ernie-1.0', \n",
    "                                                 num_labels=len(ner_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 模型创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_module = GlobalPointerBert.from_pretrained('nghuyong/ernie-1.0', \n",
    "                                              config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 三、任务构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 任务参数和必要部件设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行次数\n",
    "num_epoches = 1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_default_model_optimizer(dl_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 任务创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task(dl_module, optimizer, 'gpce', cuda_device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(ner_train_dataset, \n",
    "          ner_dev_dataset,\n",
    "          lr=5e-5,\n",
    "          epochs=7, \n",
    "          batch_size=batch_size,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 四、生成提交数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ark_nlp.model.ner.global_pointer_bert import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_predictor_instance = Predictor(model.module, tokenizer, ner_train_dataset.cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_json('../../data/source_datasets/CMeIE/CMeIE_test.json', lines=True)\n",
    "\n",
    "submit = []\n",
    "for _text in test_df['text'].to_list():\n",
    "    submit.append({\n",
    "        'text': _text,\n",
    "        'entities': ner_predictor_instance.predict_one_sample(_text)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../../data/output_datasets/CMeIE_test_entities.json'\n",
    "\n",
    "with open(output_path,'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(submit, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [transformers]",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
