{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from ark_nlp.model.tm.sbert import Module\n",
    "from ark_nlp.model.tm.sbert import ModuleConfig\n",
    "from ark_nlp.model.tm.sbert import Dataset\n",
    "from ark_nlp.model.tm.sbert import Task\n",
    "from ark_nlp.model.tm.sbert import get_default_model_optimizer\n",
    "from ark_nlp.model.tm.sbert import Tokenizer\n",
    "from ark_nlp.factory.utils.seed import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目录地址\n",
    "# 数据集下载地址：http://icrc.hitsz.edu.cn/info/1037/1146.htm\n",
    "\n",
    "train_data_path = '../data/source_datasets/LCQMC/train.txt'\n",
    "dev_data_path = '../data/source_datasets/LCQMC/dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nghuyong/ernie-1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 一、数据读入与处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.read_csv(train_data_path, sep='\\t', names=['text_a', 'text_b', 'label'])\n",
    "train_data_df = (train_data_df\n",
    "                 .loc[:,['text_a', 'text_b', 'label']])\n",
    "\n",
    "dev_data_df = pd.read_csv(dev_data_path, sep='\\t', names=['text_a', 'text_b', 'label'])\n",
    "dev_data_df = (dev_data_df\n",
    "                 .loc[:,['text_a', 'text_b', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_train_dataset = Dataset(train_data_df)\n",
    "sbert_dev_dataset = Dataset(dev_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 词典创建和生成分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载分词器\n",
    "tokenizer = Tokenizer(vocab=model_name, max_seq_len=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_train_dataset.convert_to_ids(tokenizer)\n",
    "sbert_dev_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 二、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = ModuleConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 模型创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_module = Module.from_pretrained(\n",
    "    model_name,\n",
    "    config=bert_config,\n",
    "    dropout=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 三、任务构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 任务参数和必要部件设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行次数\n",
    "num_epoches = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(dl_module.named_parameters())\n",
    "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 任务创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task(dl_module, 'adamw', 'ce', cuda_device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    sbert_train_dataset,\n",
    "    sbert_dev_dataset,\n",
    "    lr=1e-5,\n",
    "    epochs=1,\n",
    "    batch_size=64,\n",
    "    params=optimizer_grouped_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 四、模型验证与保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ark_nlp.model.tm.unsupervised_simcse import Predictor\n",
    "\n",
    "sbert_predictor_instance = Predictor(model.module, tokenizer, sbert_dev_dataset.cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_predictor_instance.predict_one_sample(['感冒', '恐惧'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_predictor_instance.predict_one_sample(['感冒', '恐惧'], return_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_predictor_instance.predict_one_sample(['感冒', '恐惧'], threshold=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Batch模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = '../data/source_datasets/LCQMC/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = pd.read_csv(test_data_path, sep='\\t', names=['text_a', 'text_b', 'label'])\n",
    "test_data_df = (test_data_df\n",
    "                 .loc[:,['text_a', 'text_b', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_test_dataset = Dataset(test_data_df, categories=sbert_train_dataset.categories, is_test=True)\n",
    "sbert_test_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_label = sbert_predictor_instance.predict_batch(sbert_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 多样本验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = '../data/source_datasets/LCQMC/test.txt'\n",
    "test_data_df = pd.read_csv(test_data_path, sep='\\t', names=['text_a', 'text_b', 'label'])\n",
    "test_data_df = (test_data_df\n",
    "                 .loc[:,['text_a', 'text_b', 'label']])\n",
    "\n",
    "record_ = []\n",
    "for _text_a, _text_b in zip(test_data_df['text_a'], test_data_df['text_b']):\n",
    "    record_.append([_text_a, _text_b, sbert_predictor_instance.predict_one_sample([_text_a, _text_b])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [transformers]",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}