{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import codecs\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from ark_nlp.model.re.gplinker_bert import Module\n",
    "from ark_nlp.model.re.gplinker_bert import ModuleConfig\n",
    "from ark_nlp.model.re.gplinker_bert import Dataset\n",
    "from ark_nlp.model.re.gplinker_bert import Task\n",
    "from ark_nlp.model.re.gplinker_bert import get_default_model_optimizer\n",
    "from ark_nlp.model.re.gplinker_bert import Tokenizer\n",
    "from ark_nlp.factory.utils.seed import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目录地址\n",
    "# 数据集下载地址：https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414\n",
    "\n",
    "train_data_path = '../data/source_datasets/CMeIE/CMeIE_train.json'\n",
    "dev_data_path = '../data/source_datasets/CMeIE/CMeIE_dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'freedomking/ernie-1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、数据读入与处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = []\n",
    "\n",
    "with codecs.open(train_data_path, mode='r', encoding='utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    for index_, line_ in enumerate(lines):\n",
    "        record_ = {}\n",
    "        line_ = json.loads(line_.strip())\n",
    "        record_['text'] = line_['text']\n",
    "        record_['label'] = []\n",
    "        for triple_ in line_['spo_list']:\n",
    "            record_['label'].append([\n",
    "                triple_['subject'],\n",
    "                record_['text'].index(triple_['subject']),\n",
    "                record_['text'].index(triple_['subject'])+ len(triple_['subject']) - 1,\n",
    "                triple_['predicate'] + '@' + triple_['object_type']['@value'],\n",
    "                triple_['object']['@value'],\n",
    "                record_['text'].index(triple_['object']['@value']),\n",
    "                record_['text'].index(triple_['object']['@value']) + len(triple_['object']['@value']) - 1,\n",
    "            ])\n",
    "        train_data_list.append(record_)\n",
    "\n",
    "train_df = pd.DataFrame(train_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data_list = []\n",
    "counter = 0\n",
    "with codecs.open(dev_data_path, mode='r', encoding='utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    for index_, line_ in enumerate(lines):\n",
    "        record_ = {}\n",
    "        line_ = json.loads(line_.strip())\n",
    "        record_['text'] = line_['text']\n",
    "        record_['label'] = []\n",
    "        for triple_ in line_['spo_list']:\n",
    "            record_['label'].append([\n",
    "                triple_['subject'],\n",
    "                record_['text'].index(triple_['subject']),\n",
    "                record_['text'].index(triple_['subject'])+ len(triple_['subject']) - 1,\n",
    "                triple_['predicate'] + '@' + triple_['object_type']['@value'],\n",
    "                triple_['object']['@value'],\n",
    "                record_['text'].index(triple_['object']['@value']),\n",
    "                record_['text'].index(triple_['object']['@value']) + len(triple_['object']['@value']) - 1,\n",
    "            ])\n",
    "            counter += 1\n",
    "        dev_data_list.append(record_)\n",
    "        \n",
    "dev_df = pd.DataFrame(dev_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_train_dataset = Dataset(train_df)\n",
    "re_dev_dataset = Dataset(dev_df,\n",
    "                         categories = re_train_dataset.categories,\n",
    "                         is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 词典创建和生成分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab=model_name, max_seq_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_train_dataset.convert_to_ids(tokenizer)\n",
    "re_dev_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 二、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = ModuleConfig.from_pretrained(model_name, num_labels=len(re_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 模型创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_type_num 实体类型, 默认值为2, 即头实体和尾实体 \n",
    "# relation_type_num 关系类型, 一般是使用头实体类型+关系类型+尾实体类型, 默认值为2\n",
    "dl_module = Module.from_pretrained(model_name,\n",
    "                                  config=bert_config,\n",
    "                                  entity_type_num=2, \n",
    "                                  relation_type_num=len(re_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 三、任务构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 任务参数和必要部件设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_default_model_optimizer(dl_module) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 任务创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新版的 loss 有默认的选项，所以这里可以设置为None\n",
    "model = Task(dl_module, optimizer, None, cuda_device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新版的 fit 参数还包含 task_utils 中的所有参数，\n",
    "# 主要用来控制 结果打印 和 模型保存 的条件\n",
    "\n",
    "# 部分问题:\n",
    "\n",
    "# Q1: 训练时 train_loss 大于 loss？\n",
    "# A1: 新版的 train loss 和 loss 计算公式不同。\n",
    "\n",
    "# 注意：evaluate_during_training_step 等于0 时不会自动保存最佳模型。\n",
    "\n",
    "model.fit(re_train_dataset,\n",
    "          re_dev_dataset,\n",
    "          epoch_num=10\n",
    "          batch_size=32,\n",
    "          evaluate_during_training_step=200,\n",
    "          do_save_best_module=True,\n",
    "          save_best_module_metric='f1-score',\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 四、模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ark_nlp.model.re.gplinker_bert import Predictor\n",
    "\n",
    "gpliner_re_predictor_instance = Predictor(model.module, tokenizer, re_train_dataset.cat2id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '骨性关节炎@在其他关节（如踝关节和腕关节），骨性关节炎比较少见，并且一般有潜在的病因（如结晶性关节病、创伤）'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpliner_re_predictor_instance.predict_one_sample(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 多样本验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = '../data/source_datasets/CMeIE/CMeIE_test.json'\n",
    "schemas_data_path = '../data/source_datasets/CMeIE/53_schemas.json'\n",
    "output_data_path = '../data/output_datasets/CMeIE_test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "with open(test_data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line_ in lines:\n",
    "        result.append(gpliner_re_predictor_instance.predict_one_sample(eval(line_)['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_subject_type = []\n",
    "all_predicate = []\n",
    "all_shcemas = []\n",
    "predicate2subject = {}\n",
    "with open(schemas_data_path, 'r', encoding='utf-8') as fs:\n",
    "    for jsonstr in fs.readlines():\n",
    "        jsonstr = json.loads(jsonstr)\n",
    "        # all_shcemas.append(jsonstr)\n",
    "        \n",
    "        predicate2subject[jsonstr['predicate']+'@'+jsonstr['object_type']] = jsonstr['subject_type']\n",
    "        \n",
    "    fs.close()\n",
    "    \n",
    "all_predicate = set(all_predicate)\n",
    "with open(output_data_path, 'w', encoding='utf-8') as fw:\n",
    "    with open(test_data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for index_, jsonstr in tqdm(enumerate(lines)):\n",
    "            line = json.loads(jsonstr)\n",
    "            results_len = []\n",
    "            sentence = line['text']\n",
    "            dict_list = result[index_]\n",
    "            new = []\n",
    "            for list_ in dict_list:\n",
    "                for predicate_ in predicate2subject:\n",
    "                    if list_[1] == predicate_:\n",
    "                        if list_[-1] != '' and list_[-1] != '[UNK]':\n",
    "                            result_dict = {\n",
    "                                'predicate': predicate_.split('@')[0],\n",
    "                                \"subject\": list_[0],\n",
    "                                'subject_type': predicate2subject[predicate_],\n",
    "                                \"object\": {\"@value\": list_[-1]},\n",
    "                                'object_type': {\"@value\":predicate_.split('@')[-1]}\n",
    "                                }\n",
    "                        else:\n",
    "                            continue\n",
    "                        if result_dict not in new:\n",
    "                            new.append(result_dict)\n",
    "            if sum([item.count('。') for item in sentence]) >= 2:\n",
    "                for item in new:\n",
    "                    item['Combined'] = True\n",
    "            else:\n",
    "                for item in new:\n",
    "                    item['Combined'] = False\n",
    "\n",
    "            if len(new) == 0:\n",
    "                new = [{\n",
    "                    \"Combined\": '',\n",
    "                    \"predicate\": '',\n",
    "                    \"subject\": '',\n",
    "                    \"subject_type\": '',\n",
    "                    \"object\": {\"@value\": \"\"},\n",
    "                    \"object_type\": {\"@value\": \"\"},\n",
    "                }]\n",
    "                pred_dict = {\n",
    "                    \"text\": ''.join(sentence),\n",
    "                    \"spo_list\": new,\n",
    "                }\n",
    "            else:\n",
    "\n",
    "                pred_dict = {\n",
    "                    \"text\": ''.join(sentence),\n",
    "                    \"spo_list\": new,\n",
    "                }\n",
    "            fw.write(json.dumps(pred_dict, ensure_ascii=False) + '\\n')\n",
    "f.close()\n",
    "fw.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [transformers]",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
